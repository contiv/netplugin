# -*- mode: ruby -*-
# vi: set ft=ruby :

require File.join(File.dirname(__FILE__), "..", "vagrant_version_check")

require "rubygems"
require "json"
require 'fileutils'

# netplugin_synced_gopath="/opt/golang"
gopath_folder="/opt/gopath"

http_proxy = ENV['HTTP_PROXY'] || ENV['http_proxy'] || ''
https_proxy = ENV['HTTPS_PROXY'] || ENV['https_proxy'] || ''

# python -c 'import random; print "%0x.%0x" % (random.SystemRandom().getrandbits(3*8), random.SystemRandom().getrandbits(8*8))
token = "d900e1.8a392798f13b33a4"
master_ip = "192.168.2.10"

# method to create an etc_hosts file based on the cluster info
def create_etc_hosts(cluster)
  if ENV['VAGRANT_CWD'] then
     folder = ENV['VAGRANT_CWD'] + "/export/.etc_hosts"
     etc_file = open(folder, "w")
  else
     etc_file = open("./export/.etc_hosts", "w")
  end
  etc_file.write("127.0.0.1   localhost\n")

  cluster.each do |role, member_list|
    member_list.each do |member_info|
      etc_file.write("#{member_info['contiv_control_ip']}   #{member_info['name']}\n")
      if role == "master" then
        etc_file.write("#{member_info['contiv_control_ip']}   netmaster \n")
        master_ip = member_info['contiv_control_ip']
      end
    end
  end

  etc_file.close
end

provision_quagga = <<SCRIPT
## setup the environment file. Export the env-vars passed as args to 'vagrant up'
echo Args passed: [[ $@ ]]
echo "export http_proxy='$1'" >> /etc/profile.d/envvar.sh
echo "export https_proxy='$2'" >> /etc/profile.d/envvar.sh
source /etc/profile.d/envvar.sh
SCRIPT

install_contiv = <<SCRIPT
kubectl apply -f /shared/.contiv.yaml
contiv_version=$(grep -m 1  "contiv/netplugin:" /shared/.contiv.yaml | awk -F ":" '{print $3}' | xargs)
curl -L -O https://github.com/contiv/netplugin/releases/download/$contiv_version/netplugin-$contiv_version.tar.bz2
tar xf netplugin-$contiv_version.tar.bz2 netctl
rm -f netplugin-$contiv_version.tar.bz2
chmod +x netctl
mv netctl /usr/bin/
kubectl get deployment/kube-dns -n kube-system -o json  > kube-dns.yaml
kubectl delete deployment/kube-dns -n kube-system
SCRIPT

# method to read the cluster config file
def read_cluster_config
  if ENV['VAGRANT_CWD']  then
      folder = ENV['VAGRANT_CWD'] + "/cluster_defs.json"
      defs_file = open(folder)
  else
      defs_file = open("cluster_defs.json")
  end
  defs_json = defs_file.read
  clust_cfg = JSON.parse(defs_json)
  defs_file.close
  return clust_cfg
end

provision_k8 = <<SCRIPT
echo 'export GOPATH=#{gopath_folder}' > /etc/profile.d/envvar.sh
echo 'export GOBIN=$GOPATH/bin' >> /etc/profile.d/envvar.sh
echo 'export GOSRC=$GOPATH/src' >> /etc/profile.d/envvar.sh
echo 'export PATH=$PATH:/usr/local/go/bin:$GOBIN' >> /etc/profile.d/envvar.sh
echo "export http_proxy='$1'" >> /etc/profile.d/envvar.sh
echo "export https_proxy='$1'" >> /etc/profile.d/envvar.sh
echo "export no_proxy='k8master,192.168.2.10,192.168.2.11,192.168.2.12,netmaster,localhost,127.0.0.1'" >> /etc/profile.d/envvar.sh
echo "export no_proxy='k8master,192.168.2.10,192.168.2.11,192.168.2.12,netmaster,localhost,127.0.0.1'" >> ~/.profile
source /etc/profile.d/envvar.sh
# Change ownership for gopath folder
chown vagrant #{gopath_folder}
sudo yum install -y net-tools
SCRIPT

provision_master = <<SCRIPT
echo "export https_proxy='$2'" >> /etc/profile.d/envvar.sh
echo "export http_proxy='$1'" >> ~/.profile
echo "export https_proxy='$2'" >> ~/.profile
source /etc/profile.d/envvar.sh
sudo -E bash
go get github.com/tools/godep
cd $GOPATH/src/github.com/tools
go install ./godep
sudo yum install -y net-tools
sudo yum install -y git
#git config --global http.proxy $HTTP_PROXY
SCRIPT

# begin execution here
cluster = read_cluster_config
create_etc_hosts(cluster)

Vagrant.configure(2) do |config|
  if ENV['CONTIV_NODE_OS'] && ENV['CONTIV_NODE_OS'] == "rhel7" then
    config.registration.manager = 'subscription_manager'
    config.registration.username = ENV['CONTIV_RHEL_USER']
    config.registration.password = ENV['CONTIV_RHEL_PASSWD']
  end
  if ENV['CONTIV_L3'] then
      config.vm.define "quagga1" do |quagga1|

        quagga1.vm.box = "contiv/quagga1"
        quagga1.vm.box_version = "0.0.1"
        quagga1.vm.host_name = "quagga1"
        quagga1.vm.network :private_network, ip: "192.168.2.51", virtualbox__intnet: "true", auto_config: false
        quagga1.vm.network "private_network",
                         ip: "80.1.1.200",
                         virtualbox__intnet: "contiv_orange"
        quagga1.vm.network "private_network",
                         ip: "70.1.1.2",
                         virtualbox__intnet: "contiv_blue"
        quagga1.vm.provision "shell" do |s|
          s.inline = provision_quagga
          s.args = [http_proxy, https_proxy]
        end
      end
      config.vm.define "quagga2" do |quagga2|

        quagga2.vm.box = "contiv/quagga2"
        quagga2.vm.box_version = "0.0.1"
        quagga2.vm.host_name = "quagga2"
        quagga2.vm.network :private_network, ip: "192.168.2.52", virtualbox__intnet: "true", auto_config: false
        quagga2.vm.network "private_network",
                         ip: "70.1.1.1",
                         virtualbox__intnet: "contiv_blue"
        quagga2.vm.network "private_network",
                         ip: "60.1.1.200",
                         virtualbox__intnet: "contiv_green"
        quagga2.vm.network "private_network",
                         ip: "50.1.1.200",
                         virtualbox__intnet: "contiv_yellow"

        quagga2.vm.provision "shell" do |s|
          s.inline = provision_quagga
          s.args = [http_proxy, https_proxy]
        end
      end
   end

  #config.ssh.password = 'vagrant'
  config.ssh.insert_key = false
  config.ssh.private_key_path = "./../../scripts/insecure_private_key"
  
  config.vm.synced_folder "./export", "/shared"
  cluster.each do |role, member_list|
    member_list.each do |member_info|
      config.vm.define vm_name = member_info["name"] do |c|
         if ENV['CONTIV_K8'] == "1" then
       	   c.vm.box_version = "0.0.4"
           box_name = vm_name
	    if  role == "master" then
	      c.vm.box = "contiv/k8master"
              c.vm.synced_folder "./../../","/opt/gopath/src/github.com/contiv/netplugin"
	      c.vm.provision "shell" do |s|
		s.inline = provision_k8
                s.args = [ENV["http_proxy"] || "", ENV["https_proxy"] || ""]
              end
	      c.vm.provision "shell" do |s|
                s.inline = provision_master
	        s.args = [ENV["http_proxy"] || "", ENV["https_proxy"] || ""]
              end
              c.vm.network "forwarded_port", guest: 9999, host: 9999
       	   else
	      if vm_name =="k8node-01" then
 	      	c.vm.box = "contiv/k8node01"
 	      else
 		 if vm_name == "k8node-02" then
 		    c.vm.box = "contiv/k8node02"
                  else
		    c.vm.box = "contiv/k8node03"
 		 end
 	      end	
	   end
	   c.vm.synced_folder "./../../bin","/opt/gopath/bin"
	   c.vm.provision "shell" do |s|
              s.inline = provision_k8
               s.args = [ENV["http_proxy"] || "", ENV["https_proxy"] || ""]
           end
	else
          if ENV['CONTIV_NODE_OS'] && ENV['CONTIV_NODE_OS'] == "rhel7" then
            # Download rhel7.2 box from https://access.redhat.com/downloads/content/293/ver=2/rhel---7/2.0.0/x86_64/product-software
            # Add it as rhel7 vagrant box add rhel-cdk-kubernetes-7.2-29.x86_64.vagrant-virtualbox.box --name=rhel7
            c.vm.box = "rhel7"
          else
            c.vm.box = "contiv/k8s-centos"
            c.vm.box_version = "0.0.7"
          end
	    c.vm.provision "shell" do |s|
               s.inline = provision_master
               s.args = [ENV["http_proxy"] || "", ENV["https_proxy"] || ""]
            end 
	end
	network_name = "contiv_purple"
	if ENV['CONTIV_L3'] then
	   if vm_name == "k8node-03" then
             network_name = "contiv_orange"
           end
	   if vm_name == "k8node-01" then
              network_name = "contiv_yellow"
           end
	   if vm_name == "k8node-02" then
              network_name = "contiv_green"
           end
        end
	
        # configure ip address etc
        c.vm.hostname = vm_name
        c.vm.network :private_network, ip: member_info["contiv_control_ip"], virtualbox__intnet: "true"
        c.vm.network :private_network, ip: member_info["contiv_network_ip"], virtualbox__intnet: network_name, auto_config: false
        c.vm.provider "virtualbox" do |v|

                if vm_name == "k8master" then
                  v.memory = 2048
                else
                  v.memory = 1024
                end  
                # make all nics 'virtio' to take benefit of builtin vlan tag
                # support, which otherwise needs to be enabled in Intel drivers,
                # which are used by default by virtualbox
                v.customize ['modifyvm', :id, '--nictype1', 'virtio']
                v.customize ['modifyvm', :id, '--nictype2', 'virtio']
                v.customize ['modifyvm', :id, '--nictype3', 'virtio']
                v.customize ['modifyvm', :id, '--nicpromisc2', 'allow-all']
                v.customize ['modifyvm', :id, '--nicpromisc3', 'allow-all']
                v.customize ['modifyvm', :id, '--paravirtprovider', "kvm"]
        end # v
        # RHEL box that is available with the CDK comes with an older version
        # of k8s. Uninstall it and install openvswitch.
        if ENV['CONTIV_NODE_OS'] && ENV['CONTIV_NODE_OS'] == "rhel7" then
          c.vm.provision "shell", inline: <<-EOS
            yum remove -y kubernetes-node-1.2.0-0.13.gitec7364b.el7.x86_64
            yum remove -y kubernetes-client-1.2.0-0.13.gitec7364b.el7.x86_64
            yum install -y https://cisco.box.com/shared/static/zzmpe1zesdpf270k9pml40rlm4o8fs56.rpm
            EOS
        end
        c.vm.provision "shell", inline: <<-EOS
          sudo setenforce 0
          sudo systemctl stop firewalld
          sudo /etc/init.d/network restart
          #copy the etc_hosts file we created
          sudo cp /shared/.etc_hosts /etc/hosts
          EOS
        if ENV['VAGRANT_USE_KUBEADM']
          c.vm.provision :shell, path: '../../install/k8s/cluster/bootstrap_centos.sh'
          if role == 'master'
            # Install kubernetes on master
            c.vm.provision :shell, path: '../../install/k8s/cluster/k8smaster_centos.sh', args: [token, member_info['contiv_control_ip'], 'v1.4.1']
            # Now install contiv
            c.vm.provision :shell, inline: install_contiv
          else
            # Install kubernetes on nodes
            c.vm.provision :shell, path: '../../install/k8s/cluster/k8sworker_centos.sh', args: [token, master_ip]
          end # if
        end
      end # c
    end # member_info
  end #role
end #config
#
